将文本信息转化为计算机可以处理的形式

# 词或字的表示
## 词袋模型
一个向量表示一句话或一个文档。忽略词语顺序、语法、句法要素。

每个维度的数值代表该词出现的频率。

## 词向量模型

# 文本或段落的表示

## One-hot
向量的维度是词表大小，只有一个维度是1，其他是0

```
class CountVector(object):
    def __init__(self):
        self.word_counts = Counter()

    def fit(self, text_data):
        for text in tqdm(text_data, total=len(text_data)):
            for word in jieba.cut(text):
                self.word_counts[word]  += 1

    def __transform(self, texts):
        outputs = []
        for text in tqdm(texts, total=len(txets)):
            output []
            for word in self.word_counts:
                output.append(int(word in text.split()))
            output.append(output)
        return outputs
```

## tf-idf模型
使用词频和逆文档频率的乘积作为单词的权重。考虑了词语在文章之间的重要性

```
class TfidfVectorizer(object):

    def __init__(self):
        self.word_count = {}
        self.tf_idf = {}
        self.texts = []

    def add_text(self, text):
        if word not in self.word_count:
            self.word_count[word] = 1
        else:
            self.word_count[word] += 1

    def add_texts(self, texts):
        for text in tqdm(texts, total=len(texts)):
            self.add_text(text)
            self.texts.append(text)

    def compute_tf_idf(self):
        for word, count in tqdm(self.word_count.items(), total=len(self.word_count)):
            tf = count / len(self.word_count)
            n_contain = sum([1 for text in self.texts if word in text])
            idf = math.log(len(self.word_count)/(n_contain+1))
            self.tf_idf[word] = tf*idf

    def get_sentence_td_idf(self, sentence, default_value=0):
        sentence_tf_idf = []
        
        for word in self.word_count:
            if word in sentence:
                sentence_tf_idf.append(self.tf_idf.get(word, default_value))
            else:
                sentence_tf_idf.append(default_value)
        
        return sentence_tf_idf

    def get_sentences_tf_idf(self, sentences, default_value=0)
        sentence_tf_idf = []
        for sentence in tqdm(sentences, total=len(sentences)):
            sentence_tf_idf = self.get_sentence_tf_idf(sentence, default_value)
            sentences_tf_idf.append(sentence_td_idf)
        return snetences_tf_idf
```

语义相近的单词在`表征空间`中距离相近。

## Word2vec
用一个词的上下文作为输入，来预测这个词本身
- 输入层：C*V的onehot张量，C为上下文单词个数，V代表词表大小
- 隐藏层：1*N的张量，N代表每个词向量的维度
- 输出层：形状为1*V的向量，代表表使用上下文推理中心词时，词汇表中每个候选词的得分。

### 训练样本
包括一个中心词、一个目标词和一个标签组成的三元组。目标词是中心词的上下文时认为二者意义相近，为正样本；非上下文为负样本。

### 网络定义
使用两个Embedding层分别将中心词与目标词转为词向量。两个词向量点积后求和，作为中心词与目标词的**相似度（预测值）**。**损失函数**定义为相似度与标签的二元交叉熵损失。

- `Embedding`：将离散值映射到高维空间

### Skip-gram
用一个词作输入，预测上下文。
- 输入层：一个1*V的onehot张量，V代表词表大小
- 隐藏层：1*N的张量，代表每个词的词向量维度
- 输出层：C*V的张量，C代表上下文单词的个数。每个上下文单词的位置上每个词汇表候选词的得分。

## 预训练模型

### BERT
1. **MLM**
句子中15%的词汇被随机Mark掉
2. 预测两个句子是否应该连在一起

### N-gram